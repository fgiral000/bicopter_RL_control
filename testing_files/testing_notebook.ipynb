{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = \"&4.01,-19,1204.01,1195.99@\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'&4.01,-19,1204.01,1195.99@'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.01,-19,1204.01,1195.99'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = values.strip('&@')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4.01', '-19', '1204.01', '1195.99']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista = data.split(',')\n",
    "lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_lista = [float(value) for value in lista]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.01, -19.0, 1204.01, 1195.99]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100.0, 1100.0]\n",
      "[1200.0, 1200.0]\n",
      "[1100.0, 1100.0]\n"
     ]
    }
   ],
   "source": [
    "for array in [[1100.0,1100.0], [1200.0, 1200.0], [1100.0,1100.0]]:\n",
    "    time.sleep(2)\n",
    "    print(array)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Enviando senal maxima: (2000 us)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Enviando senal maxima: (2000 us)'\n",
    "new_text = text.strip('&@')\n",
    "new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(x):\n",
    "    y = (x + 1) / 2    # Mapear el rango [-1, 1] al rango [0, 1]\n",
    "    z = y * 300 + 1000 # Escalar el rango [0, 1] al rango [1000, 1300]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1000., 1300.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(np.array([-1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_action(action):\n",
    "    return ( (action + 1) * (300/2) ) + 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "from gym_env_balancin import ControlEnv\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import serial\n",
    "\n",
    "\n",
    "arduino = serial.Serial('COM5', 9600)\n",
    "env = ControlEnv(arduino)\n",
    "\n",
    "#check_env(env)\n",
    "\n",
    "print(\"**************TESTEANDO EL ENTORNO DE GYM ************************\")\n",
    "print(\"Dimension del espacio de acciones:\", env.action_space.shape)\n",
    "print(\"Dimension del espacio de estados:\" , env.observation_space._shape)\n",
    "arduino.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ControlEnv(gym.Env):\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, arduino_port):\n",
    "        super(ControlEnv, self).__init__()\n",
    "        # Espacio de acciones continuas entre 1000 y 1500\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n",
    "        # Espacio de estados de dimensión 6\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(4,), dtype=np.float32)\n",
    "        # Frecuencia de acciones de 50Hz\n",
    "        self.action_freq = 50\n",
    "        # Time out de 200 time steps\n",
    "        self.max_steps = 200\n",
    "        \n",
    "        # Máximo reward\n",
    "        self.max_reward = 0\n",
    "        # Early stopping después de 10 time steps con el máximo reward\n",
    "        self.max_reward_steps = 20\n",
    "        self.current_reward_steps = 0\n",
    "        # Inicialización\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "        \n",
    "    def reset(self):\n",
    "        # Reiniciar el estado y el contador de time steps y reward steps\n",
    "        pass\n",
    "        ##########################################################################################################\n",
    "        #####TAMBIEN HAY QUE PONER LOS MOTORES A 0 CUANDO SE ACABA EL EPISODIO, PERO SIN NECESIDAD DE APAGAR LA CORRIENTE\n",
    "        ############################################################################################################\n",
    "\n",
    "        # Devolver el estado actual\n",
    "        return \n",
    "    \n",
    "\n",
    "        \n",
    "    def step(self, action):\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "        # Devolver el nuevo estado, la recompensa, si el episodio ha terminado y un diccionario vacío de información adicional\n",
    "        return \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    def _calculate_reward(self, state):\n",
    "        # Calcular la recompensa como la suma de los componentes del estado\n",
    "        return - (state[0] - state[2]) ** 2\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def _secondary_reward(self, action, last_action):\n",
    "        return - ( ((action[0] - last_action[0]) ** 2) + ((action[1] - last_action[1]) ** 2) )\n",
    "    \n",
    "\n",
    "    def get_observation(self, arduino):\n",
    "        \"\"\"Metodo para recibir informacion del estado desde el arduino\"\"\"\n",
    "        \n",
    "        # Wait until the start character '<' is received\n",
    "        while True:\n",
    "            if arduino.read().decode() == '<':\n",
    "                break\n",
    "\n",
    "        # Read the data until the end character '>' is received\n",
    "        data = ''\n",
    "        while True:\n",
    "            char = arduino.read().decode()\n",
    "            if char == '>':\n",
    "                arduino.write('?'.encode())\n",
    "                break\n",
    "            data += char\n",
    "\n",
    "        # Convert the data string to a list of integers\n",
    "        data_list = list(map(float, data.split(',')))\n",
    "        return data_list\n",
    "    \n",
    "\n",
    "\n",
    "    def send_action(self, action, arduino):\n",
    "        \"\"\"método para enviar la accion tomada por el agente hacia el arduino\"\"\"\n",
    "        \"Hay que de-normalizar las acciones\"\n",
    "        #action_complete = self.denormalize_action(action)\n",
    "        action_complete = action\n",
    "        actions = np.round(action_complete, decimals=1)\n",
    "        # Convert sequence to string\n",
    "        sequenceString = '$' + str(actions[0]) + str(actions[1]) + '#' # Assumes sequence is a 2D array with shape (1, 2)\n",
    "        # Send sequence to Arduino\n",
    "        arduino.write(sequenceString.encode())\n",
    "\n",
    "\n",
    "    def denormalize_action(action):\n",
    "        return ( (action + 1) * (300/2) ) + 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from gym.spaces import Box\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n",
    "# Espacio de estados de dimensión 6\n",
    "observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(4,), dtype=np.float32)\n",
    "# Frecuencia de acciones de 50Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************TESTEANDO EL ENTORNO DE GYM ************************\n",
      "Dimension del espacio de acciones: (2,)\n",
      "Dimension del espacio de estados: (4,)\n",
      "**************EJEMPLOS DE LOS ESPACIOS DE ACCIONES Y ESTADOS ************************\n",
      "Ejemplo del espacio de acciones: [ 0.2374076  -0.09095753]\n",
      "Ejemplo del espacio de estados: [-0.0182177   0.43532625 -1.1889627  -1.2886618 ]\n",
      "Ejemplo del espacio de acciones: <class 'numpy.ndarray'>\n",
      "Ejemplo del espacio de estados: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"**************TESTEANDO EL ENTORNO DE GYM ************************\")\n",
    "print(\"Dimension del espacio de acciones:\", action_space.shape)\n",
    "print(\"Dimension del espacio de estados:\" , observation_space._shape)\n",
    "\n",
    "print(\"**************EJEMPLOS DE LOS ESPACIOS DE ACCIONES Y ESTADOS ************************\")\n",
    "print(\"Ejemplo del espacio de acciones:\", action_space.sample())\n",
    "print(\"Ejemplo del espacio de estados:\" , observation_space.sample())\n",
    "print(\"Ejemplo del espacio de acciones:\", type(action_space.sample()))\n",
    "print(\"Ejemplo del espacio de estados:\" , type(observation_space.sample()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_action(action):\n",
    "    return ( (action + 1) * (300/2) ) + 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1111.4371, 1109.1216], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denormalize_action(action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\dgtss\\AppData\\Local\\Microsoft\\WindowsApps\\python3.11.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/dgtss/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "np.round(np.array([1111.4371, 1109.1216], dtype=np.float32), decimals=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_timesteps': '5e5', 'policy': 'MlpPolicy', 'batch_size': '256', 'learning_rate': 'lin_7.3e-4', 'buffer_size': '1000000', 'ent_coef': 'auto', 'gamma': '0.99', 'tau': '0.01', 'train_freq': '1', 'gradient_steps': '1', 'learning_starts': '10000', 'policy_kwargs': 'dict(net_arch=[400, 300])'}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "with open('hyperparam.yaml') as f:\n",
    "        config = yaml.load(f, Loader=yaml.BaseLoader)  # config is dict\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "#import tensorflow as tf\n",
    "import tensorboard\n",
    "import numpy as np\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import Logger\n",
    "\n",
    "class TensorboardCallback(BaseCallback):\n",
    "    def __init__(self, verbose=1):\n",
    "        super(TensorboardCallback, self).__init__(verbose)\n",
    "        self.primary_reward = 0\n",
    "        self.secondary_reward = 0\n",
    "        self.log_path = './sac_testing_v0'\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        pass\n",
    "\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "\n",
    "        # Sacar los datos del env\n",
    "        self.primary_reward = self.training_env.get_attr('reward_1')[0]\n",
    "        self.time_reward = self.training_env.get_attr('reward_2')[0]\n",
    "        self.goal_reward = self.training_env.get_attr('reward_3')[0]\n",
    "        self.timeout_reward = self.training_env.get_attr('reward_4')[0]\n",
    "\n",
    "        self.space_state = self.training_env.get_attr('current_state')[0]\n",
    "        self.action = self.training_env.get_attr('last_action')[0]\n",
    "        self.max_theta = self.training_env.get_attr('max_theta')[0]\n",
    "        self.max_aceleration = self.training_env.get_attr('max_aceleration')[0]\n",
    "        # current_step = self.training_env.get_attr('current_step')[0]\n",
    "\n",
    "        # Espacio de los estados\n",
    "        state_space = np.array(self.space_state)\n",
    "\n",
    "        theta = state_space[0]\n",
    "        self.logger.record(\"state_space/theta\", theta)\n",
    "\n",
    "        theta_denorm = theta*self.max_theta\n",
    "        self.logger.record(\"state_space/theta_denorm\", theta_denorm)\n",
    "\n",
    "\n",
    "        theta_dot = state_space[1]\n",
    "        self.logger.record(\"state_space/theta_dot\", theta_dot)\n",
    "\n",
    "        theta_dot_denorm = theta_dot*self.max_aceleration\n",
    "        self.logger.record(\"state_space/theta_dot_denorm\", theta_dot_denorm)\n",
    "\n",
    "\n",
    "        # Espacio de acciones\n",
    "\n",
    "        actions = np.array(self.action)\n",
    "        actions_denorm = self.denormalize_action(actions)\n",
    "\n",
    "        Ti = actions[0]\n",
    "        self.logger.record(\"action_space/Left_Thrust\", Ti)\n",
    "\n",
    "        Ti_denorm = actions_denorm[0]\n",
    "        self.logger.record(\"action_space/Left_Thrust_Denorm\", Ti_denorm)\n",
    "\n",
    "        Td = actions[1]\n",
    "        self.logger.record(\"action_space/Right_Thrust\", Td)\n",
    "\n",
    "        Td_denorm = actions_denorm[1]\n",
    "        self.logger.record(\"action_space/Right_Thrust_Denorm\", Td_denorm)\n",
    "\n",
    "        # Rewards\n",
    "        self.total_reward = self.primary_reward + self.time_reward + self.goal_reward + self.timeout_reward\n",
    "\n",
    "        self.logger.record(\"rewards/total_reward\", self.total_reward)\n",
    "\n",
    "        self.logger.record(\"rewards/primary_reward\", self.primary_reward)\n",
    "\n",
    "        self.logger.record(\"rewards/time_reward\", self.time_reward)\n",
    "\n",
    "        self.logger.record(\"rewards/goal_reward\", self.goal_reward)\n",
    "\n",
    "        self.logger.record(\"rewards/timeout_reward\", self.timeout_reward)\n",
    "\n",
    "\n",
    "        # Imprimo todo\n",
    "        self.logger.dump(self.num_timesteps)\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "    def denormalize_action(self, action):\n",
    "        return ( (action + 1) * (300/2) ) + 1000\n",
    "\n",
    "\n",
    "class ControlEnv(gym.Env):\n",
    "\n",
    "    def __init__(self, arduino_port):\n",
    "        super(ControlEnv, self).__init__()\n",
    "        # Espacio de acciones continuas entre 1000 y 1500\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n",
    "        # Espacio de estados de dimensión 6\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32)\n",
    "        # Frecuencia de acciones de 50Hz\n",
    "        self.action_freq = 50\n",
    "        # Time out de 200 time steps\n",
    "        self.max_steps = 500\n",
    "\n",
    "        # Máximo reward\n",
    "        self.max_reward = 0.0\n",
    "        # Early stopping después de 10 time steps con el máximo reward\n",
    "        self.max_reward_steps = 250\n",
    "        self.current_reward_steps = 0\n",
    "\n",
    "        self.counter = 0            # Numero de steps consecutivos dentro de la zona buena\n",
    "\n",
    "\n",
    "        ###Valores maximos de los parametros del vector de estado\n",
    "        self.max_theta = 30\n",
    "        self.max_aceleration = 20000/131\n",
    "\n",
    "\n",
    "        #Valores del vector de estados\n",
    "        ##############################################################################\n",
    "        #AQUI SE DEBEN INTRODUCIR LOS VALORES LEIDOS DEL ARDUINO ANTES DE EMPEZAR EL EPISODIO\n",
    "        ###################################################################################3\n",
    "        self.theta_referencia = 0.0\n",
    "        self.theta_inicial = None\n",
    "        self.theta_aceleracion_inicial = None\n",
    "        self.current_step = 0\n",
    "\n",
    "        # Estado actual\n",
    "        self.current_state = np.array([self.theta_inicial, self.theta_aceleracion_inicial, self.theta_referencia, self.current_step, self.last_action[0],self.last_action[1]], dtype=np.float32)\n",
    "\n",
    "\n",
    "        self.last_action = None\n",
    "        self.arduino_values = None\n",
    "\n",
    "        self.previous_shaping = None\n",
    "        self.arduino_port = arduino_port\n",
    "        self.arduino_port.reset_input_buffer()\n",
    "\n",
    "        # Inicialización\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self,seed=42):\n",
    "        # Reiniciar el estado y el contador de time steps y reward steps\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.current_reward_steps = 0\n",
    "\n",
    "        self.last_action = np.array([-1,-1], dtype=np.float32)\n",
    "\n",
    "        self.send_action(np.array([-1,-1], dtype=np.float32), self.arduino_port)\n",
    "        #Se deben reiniciar tambien los valores de estado iniciales\n",
    "        self.arduino_values = self.get_observation(self.arduino_port)\n",
    "        ##############################################################################\n",
    "        #AQUI SE DEBEN INTRODUCIR LOS VALORES LEIDOS DEL ARDUINO ANTES DE EMPEZAR EL EPISODIO\n",
    "        ###################################################################################3\n",
    "        # Valores del vector de estados\n",
    "        self.theta_referencia = 0.0\n",
    "        self.theta_inicial = self.arduino_values[0] \n",
    "        self.theta_aceleracion_inicial = self.arduino_values[1] \n",
    "        self.current_step = 0\n",
    "\n",
    "        self.previous_shaping = None\n",
    "        self.current_state = np.array([self.theta_inicial, self.theta_aceleracion_inicial, self.theta_referencia, self.current_step, self.last_action[0],self.last_action[1]], dtype=np.float32)\n",
    "\n",
    "        # self.current_state = np.array(self.current_state, dtype=np.float32)\n",
    "\n",
    "        self.arduino_port.reset_input_buffer()\n",
    "        self.arduino_port.reset_output_buffer()\n",
    "        ##########################################################################################################\n",
    "        #####TAMBIEN HAY QUE PONER LOS MOTORES A 0 CUANDO SE ACABA EL EPISODIO, PERO SIN NECESIDAD DE APAGAR LA CORRIENTE\n",
    "        ############################################################################################################\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Devolver el estado actual\n",
    "        return self.current_state\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        ####### ENVIAR ACCCION TOMADA AL ARDUINO ########\n",
    "        self.send_action(action, self.arduino_port)\n",
    "\n",
    "\n",
    "        # Incrementar el contador de time steps\n",
    "        self.current_step += 1\n",
    "\n",
    "\n",
    "        # Calcular el nuevo estado a partir de la acción y el estado actual\n",
    "        ##### CALCULAR ESTADO DESDE EL ARDUINO ########\n",
    "        new_arduino_data = self.get_observation(self.arduino_port)\n",
    "        ########################################################################################\n",
    "        # AQUI SE DEBEN INTRODUCIR LOS VALORES LEIDOS DEL ARDUINO ANTES DE EMPEZAR EL EPISODIO #\n",
    "        ########################################################################################\n",
    "\n",
    "        new_state = np.array([new_arduino_data[0], new_arduino_data[1], self.theta_referencia, self.current_step, action[0], action[1]], dtype=np.float32)\n",
    "\n",
    "        # ---------------------------- RECOMPENSAS -------------------------\n",
    "\n",
    "        reward = - abs(new_state[0] - new_state[2])\n",
    "\n",
    "        shaping = (\n",
    "\n",
    "                    -100 * abs(new_state[0] - new_state[2])\n",
    "\n",
    "                    -10 * self.current_step\n",
    "\n",
    "                    + (10 - abs(new_state[0] - new_state[2])) * (abs(new_state[0] - new_state[2]) <= 1) \n",
    "\n",
    "                )\n",
    "        \n",
    "        if self.previous_shaping is not None:\n",
    "\n",
    "            reward = shaping - self.previous_shaping\n",
    "\n",
    "        self.previous_shaping = shaping\n",
    "        # -------------------------------------------------------------------\n",
    "\n",
    "        #Se guarda la ultima accion tomada para utilizarla en el reward\n",
    "        self.last_action = action\n",
    "\n",
    "        # Actualizar el contador de reward steps si se alcanza el máximo reward\n",
    "        if abs(new_state[0] - new_state[2]) <= 1:\n",
    "            self.current_reward_steps += 1\n",
    "        else:\n",
    "            self.current_reward_steps = 0\n",
    "\n",
    "        # Comprobar si se ha alcanzado el time out o el early stopping\n",
    "        if self.current_reward_steps == self.max_reward_steps:\n",
    "            reward+=100\n",
    "            done = True\n",
    "\n",
    "        if self.current_step == self.max_steps:\n",
    "            reward-=100\n",
    "            done = True\n",
    "\n",
    "\n",
    "        # Actualizar el estado actual\n",
    "        self.current_state = new_state\n",
    "\n",
    "        # Devolver el nuevo estado, la recompensa, si el episodio ha terminado y un diccionario vacío de información adicional\n",
    "        return self.current_state, reward, done, {}\n",
    "\n",
    "\n",
    "\n",
    "    def _calculate_reward(self, state):\n",
    "        # Calcular la recompensa como la suma de los componentes del estado\n",
    "        return np.clip((- (state[0] - state[2]) ** 2 + 9/625)*625*10/9, -20, 20)\n",
    "\n",
    "    def _time_reward(self):\n",
    "        #  Cada paso que des quitas 1 de recompensa, para incentivar que se de prisa\n",
    "        return -1\n",
    "\n",
    "    def _goal_reached(self):\n",
    "\n",
    "        if self.reward_1 >= 0:\n",
    "            self.counter+=1\n",
    "        else:\n",
    "            self.counter = 0\n",
    "\n",
    "        if self.counter >= 100:\n",
    "            reward = 100\n",
    "            done = True\n",
    "            self.counter = 0\n",
    "\n",
    "        else:\n",
    "            reward = 0\n",
    "            done = False\n",
    "\n",
    "        return reward, done\n",
    "\n",
    "\n",
    "    def _timeout(self):\n",
    "        if self.current_step >= self.max_steps:\n",
    "            done = True\n",
    "            reward = -100\n",
    "        else:\n",
    "            done = False\n",
    "            reward = 0\n",
    "\n",
    "        return reward, done\n",
    "\n",
    "\n",
    "    def _secondary_reward(self, action, last_action):\n",
    "        return - ( ((action[0] - last_action[0]) ** 2) + ((action[1] - last_action[1]) ** 2) )\n",
    "\n",
    "\n",
    "    def get_observation(self, arduino):\n",
    "        \"\"\"Metodo para recibir informacion del estado desde el arduino\"\"\"\n",
    "\n",
    "        # Wait until the start character '<' is received\n",
    "        while True:\n",
    "            if arduino.read().decode() == '<':\n",
    "                break\n",
    "\n",
    "        # Read the data until the end character '>' is received\n",
    "        data = ''\n",
    "        while True:\n",
    "            char = arduino.read().decode()\n",
    "            if char == '>':\n",
    "                arduino.write('?'.encode())\n",
    "                break\n",
    "            data += char\n",
    "\n",
    "        # Convert the data string to a list of integers\n",
    "        data_list = list(map(float, data.split(',')))\n",
    "        return data_list\n",
    "\n",
    "\n",
    "\n",
    "    def send_action(self, action, arduino):\n",
    "        \"\"\"método para enviar la accion tomada por el agente hacia el arduino\"\"\"\n",
    "        \"Hay que de-normalizar las acciones\"\n",
    "        action_complete = self.denormalize_action(action)\n",
    "\n",
    "        actions = np.round(action_complete, decimals=1)\n",
    "        # Convert sequence to string\n",
    "        sequenceString = '$' + str(actions[0]) + str(actions[1]) + '#' # Assumes sequence is a 2D array with shape (1, 2)\n",
    "        # Send sequence to Arduino\n",
    "        arduino.write(sequenceString.encode())\n",
    "\n",
    "\n",
    "    def denormalize_action(self, action):\n",
    "        return ( (action + 1) * (300/2) ) + 1050\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returned(state, t,previous_shaping = None):\n",
    "\n",
    "    reward = - abs(state[0] - state[2])\n",
    "\n",
    "    shaping = (\n",
    "\n",
    "        -100 * abs(state[0] - state[2])\n",
    "\n",
    "        -10 * t\n",
    "\n",
    "        + (10 - abs(state[0] - state[2])) * (abs(state[0] - state[2]) <= 1) \n",
    "\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(10 - abs(3 - 2) ) * (abs(3 - 2) <= 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(3 - 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.005"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+0.005-0.005*0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_uav_control",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e6cf8e7033156a132c693813ce109798b4bc5fbbd2fee71dbf3e6a3090fea77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
